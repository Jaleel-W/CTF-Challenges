# ğŸ›¡ï¸ AI Security Lab: Adversarial Attack & Defense Simulation

> A practical exploration of evasion attacks and defense strategies for image classification models, inspired by real-world AI security research.

---

## ğŸ“– Overview

This repository documents a lab exercise from **CSCI 400**, exploring how adversarial attacks can fool AI models and how to defend against them. The project includes:

- An attack scenario: **"The Wolf in the Sheep's Clothing"** â€” fooling a CNN into misclassifying a wolf as a sheep.
- A comprehensive defense strategy with short-, medium-, and long-term mitigations.
- Implementations of adversarial training, input sanitization, ensemble methods, and more.

---

## ğŸ¯ Attack Scenario

**Objective:**  
Cause a high-accuracy CNN (e.g., trained on ImageNet) to misclassify a wolf image as a **sheep**.

**Method:**  
Evasion attack using adversarial examples â€” carefully perturbed inputs designed to deceive the model.

---

## ğŸ›¡ï¸ Defense Strategies

### Immediate Response (0â€“24 hours)
- Collect evidence and logs
- Throttle affected endpoints
- Enable human-in-the-loop review
- Rate limiting and source blocking

### Short-Term Mitigations
- Runtime detection (input transformations, ensemble disagreement)
- Input sanitization (JPEG compression, noise addition)
- Temporary source blocking

### Medium-Term Remediation
- Adversarial training (PGD, FGSM)
- Ensemble models with cross-validation
- Randomized smoothing

### Long-Term Hardening
- Adversarial testing in CI/CD
- Permanent adversarial detection models
- Updated operational policies and robustness audits

---

## ğŸ§ª Features

- âœ… Adversarial example generation (FGSM, PGD)
- âœ… Input preprocessing & sanitization
- âœ… Ensemble model validation
- âœ… Adversarial detection system
- âœ… Randomized smoothing for certified defenses
- âœ… Human-in-the-loop review pipeline
- âœ… Continuous robustness monitoring

---

## ğŸš€ Quick Start

```bash
git clone https://github.com/your-username/ai-adversarial-lab.git
cd ai-adversarial-lab
pip install -r requirements.txt
python train_adversarial.py
ğŸ“ Repository Structure
text
â”œâ”€â”€ attacks/           # Adversarial example generators
â”œâ”€â”€ defenses/          # Mitigation strategies
â”œâ”€â”€ models/            # Pretrained and fine-tuned CNNs
â”œâ”€â”€ utils/             # Helpers for logging, transforms, etc.
â”œâ”€â”€ notebooks/         # Jupyter notebooks for demo
â””â”€â”€ tests/             # Robustness and accuracy tests
ğŸ“Œ Defense Checklist
Contain & Collect

Runtime Detection

Input Sanitization

Human Review

Adversarial Training

Ensemble Methods

CI Integration

Monitor & Communicate

ğŸ‘¥ Contributors
Built as part of CSCI 400 Lab 14

Contributions welcome! Feel free to open issues or PRs.

ğŸ“„ License
This project is licensed under the MIT License. See the LICENSE file for details.
